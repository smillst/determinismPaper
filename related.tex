\section{Related work}\label{sec:related}

\todo{TODO}

The state of the art is NonDex~\cite{nondex}.  It is a highly effective tool.
% https://github.com/TestingResearchIllinois/NonDex/wiki/Supported-APIs
NonDex uses a hand-crafted list of 47 methods (25 unique method names)
in 13 classes that were identified as potential sources of flakiness. 
For each of the identified methods, the authors built models that
return different results when called consecutively. A modified JVM then
runs a given test multiple times and reports the test as being flaky if it observes
diverging test output. While this approach produces precise results, it requires manual inspection
and considerable debugging effort to locate and fix the source of flakiness. \TheDeterminismChecker, in
contrast, reports the cause of nondeterminism at compile time requiring little to no debugging effort with the
caveat of producing higher false positives compared to NonDex. 
Identifying and modeling methods with nondeterministic
specifications is analogous to our annotations on the JDK\@. So far, we have annotated
928 methods across 41 classes.
%\todo{Check whether our JDK is missing anything they support - No}
%\todo{Did the NonDex authors contact the maintainers of the tools in which
%  they found nondeterminism - No!}
%\todo{The NonDex authors didn't contact the maintainers of tools.
%From the paper: "NonDex detected 57 previously unknown
%flaky tests in open-source projects and three flaky tests
%that have been already fixed by the open-source software
%developers."}

DeFlaker~\cite{deflaker} is another approach for flake test detection. It relies on
version control history to detect tests that behave differently. It computes a diff of the code covered
in the current version and the previous one. If there exists a test case whose code coverage does not include
this diff but still produces different results on the two versions being compared, it is flagged as being flaky.
This approach doesn't require JVM modifications
and integrates easily with a production software. DeFlaker reported 19 previously unknown bugs
in open source projects, 7 of which were addressed by the developers of these projects. DeFlaker is agnostic
to the code under test and can therefore report flakiness arising out of concurrency which \TheDeterminismChecker cannot.

Nondeterminism in tests is of interest to both
researchers and software developers alike~\cite{Fowler,Sudarshan}.
\cite{LuoHEM2014} study 51 open source projects and analyze the most common root causes of
test flakiness. The findings in the paper suggest that most of the flakiness in tests is caused by
the use of async await, concurrency, or due to test order dependency.~\cite{Plotkin:1993:LPP:645891.671433}
perform an empirical study on open source benchmarks that concur with the results of~\cite{LuoHEM2014}.
Additionally, the authors of this paper study the effects of removing these test smells. Their results suggest that
refactoring the test code smells fixed more than half of the identified flaky tests.

\cite{Jin:2012:UDR:2254064.2254075} study the effect of inefficient code sequences on the performance of software.
Their findings suggest that incorrect understanding of performance specifications of APIs is a very common root cause for
performance bugs. ~\cite{Rui:2013:180377} analyze the APIs related to security such as authorization and authentication.
These APIs which come as a part of the SDKs provided by online providers are used by application developers to 
build secure apps. The paper discovers that the security guarantees provided by these SDKs are violated due to
the implicit assumptions made by the APIs that the users are unaware of. \cite{Xiao:2014:NMC:2591062.2591177}
study the effect of nondeterminism in MapReduce programs with a specific focus on nondeterminism caused by
non-commutative reducers. While they found bugs that violated correctness due to this bug pattern,
the authors reported that several of these were harmless as they relied on an implicit assumption on data
which ensured correctness. 

\cite{Petrenko1996,Petrenko:1993:NSM:648128.761244,Savor:1997:639710,Hierons:2004:TCD:1040993.1040998} propose several techniques to test whether a
deterministic implementation conforms to its non-deterministic finite state machine.
\cite{Cook:2013:RNP:2491956.2491969} present an approach that can automatically verify properties
in branching time temporal logic systems that are inherently non-deterministic.

Failing tests that are unrelated to code changes can be expensive both in terms of monetary costs and
developer effort. \cite{Herzig:2015:EDF:2819009.2819018} propose techniques to classify tests as false alarms
if they are known to be caused by testing infrastructure or other environment issues.~\cite{Huo:2014:IOQ:2635868.2635917}
propose an approach that detects brittle assertions in tests by performing a taint analysis on inputs classified as 
controlled and uncontrolled.~\cite{ZhangJWMLEN2014} investigate the effects of test independence assumption
on other techniques such as test prioritization, selection, etc. Other approaches~\cite{BellKMD2015,Gyori:2015:RTD:2771783.2771793}
analyze test dependencies and either prevent them or use this information for other optimizations.
\cite{Dan:2013:10.1007/978-3-642-39038-8_25,Vahabzadeh:2015:7332456} focus on differentiating bugs due to tests
from those caused by source code.

%\todo{A NonDet annotation:
%  \url{http://www.swi-prolog.org/pldoc/man?section=modes}}

%% Added this info in the experiments section
%\todo{All nondeterminism that NonDex found was due to 7 methods in 3 classes:
%Class\#getDeclaredFields
%Class\#getDeclaredMethods
%Class\#getFields
%DateFormatSymbols\#getZoneStrings
%HashMap\#entrySet
%HashMap\#keySet
%HashMap\#values
%}
